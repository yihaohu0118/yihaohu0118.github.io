<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AFRO: Bootstrap Dynamic-Aware 3D Visual Representation</title>
    <meta name="description" content="Project page for AFRO: Bootstrap Dynamic-Aware 3D Visual Representation for Scalable Robot Learning">
    
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&family=Noto+Sans+SC:wght@400;700&display=swap" rel="stylesheet">

    <style>
        :root {
            --primary-color: #2563eb; /* Bright Blue */
            --secondary-color: #1e40af; /* Darker Blue */
            --text-color: #1f2937; /* Dark Gray */
            --light-gray: #f3f4f6;
            --white: #ffffff;
            --max-width: 1000px;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', sans-serif;
            color: var(--text-color);
            background-color: var(--white);
            line-height: 1.6;
            margin: 0;
            padding: 0;
        }

        /* Layout Utilities */
        .container {
            max-width: var(--max-width);
            margin: 0 auto;
            padding: 2rem 1rem;
        }

        .text-center { text-align: center; }
        
        .section-title {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 2rem;
            margin-top: 4rem;
            color: #111827;
            border-bottom: 2px solid var(--light-gray);
            padding-bottom: 0.5rem;
            display: inline-block;
        }

        .subsection-title {
            font-size: 1.5rem;
            font-weight: 600;
            color: #374151;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            border-left: 4px solid var(--primary-color);
            padding-left: 1rem;
        }

        /* Header */
        header { padding: 4rem 0 2rem; }

        h1.paper-title {
            font-size: 2.5rem;
            font-weight: 800;
            line-height: 1.2;
            margin-bottom: 1.5rem;
            background: -webkit-linear-gradient(45deg, #1e3a8a, #3b82f6);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .authors { font-size: 1.1rem; margin-bottom: 1rem; }
        .authors a { color: var(--primary-color); text-decoration: none; font-weight: 500; }
        .authors a:hover { text-decoration: underline; }
        
        .affiliations { font-size: 0.95rem; color: #4b5563; margin-bottom: 2rem; }
        .affiliations span { margin: 0 0.5rem; display: inline-block; }

        /* Buttons */
        .links { display: flex; justify-content: center; gap: 1rem; margin-top: 1.5rem; flex-wrap: wrap; }
        .btn {
            background-color: var(--text-color);
            color: var(--white);
            padding: 0.75rem 1.5rem;
            border-radius: 9999px;
            text-decoration: none;
            font-weight: 600;
            transition: all 0.2s ease;
            display: inline-flex; align-items: center; gap: 0.5rem;
            box-shadow: 0 4px 6px -1px rgba(0,0,0,0.1);
        }
        .btn-primary { background-color: var(--primary-color); }
        .btn-primary:hover { background-color: var(--secondary-color); transform: translateY(-2px); }
        .btn-dark { background-color: #374151; }
        .btn-dark:hover { background-color: #111827; transform: translateY(-2px); }

        /* Images */
        .img-fluid {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 10px 15px -3px rgba(0,0,0,0.1);
            margin: 1rem 0;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .caption {
            font-size: 0.9rem;
            color: #6b7280;
            margin-top: 0.5rem;
            text-align: center;
            font-style: italic;
        }

        /* Abstract Box */
        .abstract-box {
            background-color: var(--light-gray);
            padding: 2rem;
            border-radius: 12px;
            margin: 2rem 0;
            text-align: justify;
        }

        /* Motivation Section */
        .motivation-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            margin-top: 2rem;
        }
        .motivation-card {
            background: #fff;
            border: 1px solid #e5e7eb;
            border-radius: 10px;
            padding: 1.5rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
        }
        .motivation-card h3 {
            color: #dc2626; /* Red for "Problems" */
            font-size: 1.1rem;
            margin-top: 0;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Method Grid */
        .method-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            margin-top: 2rem;
        }
        .method-item h3 { color: var(--primary-color); }

        /* Experiments */
        .exp-block { margin-bottom: 3rem; }
        
        .side-by-side-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            align-items: start;
        }

        @media (max-width: 768px) {
            .motivation-grid, .method-grid, .side-by-side-grid { grid-template-columns: 1fr; }
            h1.paper-title { font-size: 1.8rem; }
        }

        /* BibTeX */
        .bibtex-container {
            background-color: #f1f5f9;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            padding: 1.5rem;
            overflow-x: auto;
            text-align: left;
        }
        pre { margin: 0; font-family: 'Courier New', monospace; font-size: 0.9rem; color: #334155; }

        footer { margin-top: 4rem; padding: 2rem 0; border-top: 1px solid #e5e7eb; color: #6b7280; }
    </style>
</head>
<body>

    <header class="container text-center">
        <h1 class="paper-title">Bootstrap Dynamic-Aware 3D Visual <br>Representation for Scalable Robot Learning</h1>
        
        <div class="authors">
            <a href="https://kolakivy.github.io/">Qiwei Liang</a><sup>1,2,*</sup>, 
            <a href="#">Boyang Cai</a><sup>2,*</sup>, 
            <a href="#">Minghao Lai</a><sup>2,*</sup>, 
            <a href="#">Sitong Zhuang</a><sup>2,*</sup>, <br>
            <a href="https://lintao.online/">Tao Lin</a><sup>3</sup>,
            <a href="#">Yan Qin</a><sup>2</sup>, 
            <a href="#">Yixuan Ye</a><sup>4</sup>,
            <a href="#">Jiaming Liang</a><sup>2</sup>, 
            <a href="https://scholar.google.com/citations?user=Mu__bJEAAAAJ&hl=en">Renjing Xu</a><sup>1,†</sup>
        </div>

        <div class="affiliations">
            <span><sup>1</sup>HKUST (Guangzhou)</span>
            <span><sup>2</sup>Shenzhen University</span>
            <span><sup>3</sup>Beijing Jiaotong University</span>
            <span><sup>4</sup>Central South University</span>
            <br>
            <span style="font-size: 0.85rem; margin-top: 0.5rem; display: block;">*Equal Contribution &nbsp;&nbsp; †Corresponding Author</span>
        </div>

        <div class="links">
            <a href="#" class="btn btn-dark">Paper (Arxiv)</a>
            <a href="https://github.com/KolaKivy/AFRO" class="btn btn-dark">Code (GitHub)</a>
        </div>
    </header>

    <div class="container">
        <!-- Teaser -->
        <div class="text-center">
            <img src="images/teaser.png" alt="Teaser Figure showing AFRO Framework" class="img-fluid">
            <p class="caption">
                <strong>Figure 1:</strong> AFRO learns dynamics-aware 3D representations in latent space without action labels or explicit reconstruction, achieving superior performance and generalization.
            </p>
        </div>

        <!-- Abstract -->
        <section id="abstract">
            <div class="text-center"><h2 class="section-title">Abstract</h2></div>
            <div class="abstract-box">
                <p>
                    Despite strong results on recognition and segmentation, current 3D visual pre-training methods often underperform on robotic manipulation. We introduce <strong>AFRO</strong>, a self-supervised framework that learns dynamics-aware 3D representations without action or reconstruction supervision. AFRO casts state prediction as a generative diffusion process and jointly models forward and inverse dynamics in a shared latent space to capture causal transition structure. When combined with Diffusion Policy, AFRO substantially increases manipulation success rates across 16 simulated and 4 real-world tasks, outperforming existing pre-training approaches.
                </p>
            </div>
        </section>

        <!-- Motivation: The Gaps -->
        <section id="motivation">
            <div class="text-center"><h2 class="section-title" style="margin-top: 2rem; font-size: 1.5rem;">Motivation: Why Current 3D Pre-training Fails?</h2></div>
            <div class="motivation-grid">
                <div class="motivation-card">
                    <h3>
                        <svg width="24" height="24" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4m0 4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>
                        Lack of Dynamics Awareness
                    </h3>
                    <p>
                        Robotic manipulation is inherently sequential. Existing methods often rely on <strong>single-frame supervision</strong> (like reconstruction or contrastive learning on static views), completely overlooking the temporal continuity and causal dependencies (State-Action-State) essential for control.
                    </p>
                </div>
                <div class="motivation-card">
                    <h3>
                        <svg width="24" height="24" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4m0 4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>
                        Lack of Manipulation-Relevant Abstraction
                    </h3>
                    <p>
                        Current approaches often strive for <strong>holistic scene reconstruction</strong>. This forces the model to encode background details irrelevant to the task (e.g., table texture, distant walls), distracting the policy from actionable object regions and interaction dynamics.
                    </p>
                </div>
            </div>
        </section>

        <!-- Method -->
        <section id="method">
            <div class="text-center"><h2 class="section-title">Methodology</h2></div>
            
            <div class="text-center">
                <img src="images/method_overview.png" alt="Figure 2: AFRO Method Overview" class="img-fluid" style="max-width: 95%;">
                <p class="caption">
                    <strong>Figure 2:</strong> Overview of the AFRO framework. Our method learns dynamics-aware 3D representations by jointly modeling forward and inverse dynamics in a shared latent space, using feature differencing and diffusion-based state prediction.
                </p>
            </div>

            <div class="content-block">
                <p class="text-center" style="max-width: 800px; margin: 0 auto;">
                    AFRO addresses these gaps by learning <strong>dynamics-aware representations</strong>. We integrate Inverse and Forward Dynamics Models (IDM/FDM) in a latent space, utilizing feature differencing and a diffusion-based predictor.
                </p>
            </div>

            <div class="method-grid">
                <div class="method-item">
                    <h3>1. Latent Action Modeling</h3>
                    <p>To prevent feature leakage (shortcut learning), we input <strong>feature differences</strong> instead of raw feature pairs into the IDM. We also introduce <strong>Inverse-Consistency</strong> supervision to stabilize the learned dynamics.</p>
                    <!-- User requested Figure 3 here -->
                    <img src="images/figure3.png" alt="Figure 3: Inverse Dynamic Model and Feature Differencing" class="img-fluid">
                    <p class="caption"><strong>Figure 3:</strong> Feature Differencing & Inverse Dynamics.</p>
                </div>
                <div class="method-item">
                    <h3>2. Diffusion Forward Dynamics</h3>
                    <p>Since the future is multimodal (uncertain), we model forward prediction as a <strong>conditional denoising process</strong> using a Diffusion Transformer (DiT), rather than a deterministic regression.</p>
                    <!-- User requested Figure 4 here -->
                    <img src="images/figure4.png" alt="Figure 4: Forward Dynamic Model with Diffusion Transformer" class="img-fluid">
                    <p class="caption"><strong>Figure 4:</strong> Diffusion-based Forward Dynamics Model.</p>
                </div>
            </div>
        </section>

        <!-- Simulation Experiments -->
        <section id="sim-experiments">
            <div class="text-center"><h2 class="section-title">Simulation Experiments</h2></div>
            
            <!-- 1. Main Experiment -->
            <div class="exp-block">
                <h3 class="subsection-title">1. Main Benchmark Results</h3>
                <p>Comparison on 16 tasks (Adroit & MetaWorld) against SOTA baselines (DP3, PointMAE, Dynamo-3D, etc.). AFRO achieves the highest success rates.</p>
                <img src="images/results_simulation.png" alt="Table 1: Main Simulation Results" class="img-fluid" style="max-width: 85%;">
            </div>

            <!-- 2. Out-of-domain Scalability -->
            <div class="exp-block">
                <h3 class="subsection-title">2. Out-of-Domain Scalability</h3>
                <p>When pre-trained on diverse domains (multi-task), AFRO shows superior transfer capabilities compared to baselines, which often degrade or plateau.</p>
                <img src="images/figure5.png" alt="Figure 5: Domain Scalability Results" class="img-fluid" style="max-width: 85%;">
            </div>

            <!-- 3. Data Scalability -->
            <div class="exp-block">
                <h3 class="subsection-title">3. Data Scalability</h3>
                <p>Performance vs. Number of Demonstrations. AFRO continues to improve with more data, showing strong scaling laws suitable for large-scale learning.</p>
                <img src="images/figure6.png" alt="Figure 6: Data Scalability Results" class="img-fluid" style="max-width: 60%;">
            </div>

            <!-- 4. Feature Visualization -->
            <div class="exp-block">
                <h3 class="subsection-title">4. Feature Visualization (t-SNE)</h3>
                <p>Qualitative analysis of the learned latent space. AFRO forms clearer, more distinct task clusters and smooth temporal trajectories.</p>
                <img src="images/figure7.png" alt="Figure 7: t-SNE Visualization" class="img-fluid" style="max-width: 60%;">
            </div>
        </section>

        <!-- Real World Experiments -->
        <section id="real-experiments">
            <div class="text-center"><h2 class="section-title">Real-World Experiments</h2></div>

            <!-- 1. Setup & Demo -->
            <div class="exp-block">
                <h3 class="subsection-title">1. Setup & Demonstration</h3>
                <p>We evaluated on a Franka Emika arm across 4 tasks: Block Alignment, Bell Pressing, Fruit Pick-and-Place, and Cover Block.</p>
                <!-- User will stitch a picture here -->
                <img src="images/real_world_setup.png" alt="Real World Setup and Task Demonstrations" class="img-fluid">
                <p class="caption"><strong>Figure 8 & 9:</strong> Real-world experiment setup and task rollouts.</p>
            </div>

            <!-- 2. Main Real World Table -->
            <div class="exp-block">
                <h3 class="subsection-title">2. Real-World Performance</h3>
                <p>Success rates in real-world trials. AFRO significantly outperforms baselines, especially in dynamic and contact-rich tasks.</p>
                <img src="images/table2_realworld.png" alt="Table 2: Real World Performance" class="img-fluid" style="max-width: 85%;">
            </div>

            <!-- 3. Generalization (Side by Side) -->
            <div class="exp-block">
                <h3 class="subsection-title">3. Robustness & Generalization</h3>
                <div class="side-by-side-grid">
                    <div>
                        <h4 style="text-align: center;">Object Generalization</h4>
                        <p style="font-size: 0.9rem;">Testing on unseen objects (different shapes/sizes).</p>
                        <img src="images/table3_object_gen.png" alt="Table 3: Object Generalization" class="img-fluid">
                    </div>
                    <div>
                        <h4 style="text-align: center;">Cluttered Scenes</h4>
                        <p style="font-size: 0.9rem;">Testing robustness against distractor objects.</p>
                        <img src="images/table4_clutter_gen.png" alt="Table 4: Cluttered Scene Generalization" class="img-fluid">
                    </div>
                </div>
            </div>

            <!-- 4. Ablation Study -->
            <div class="exp-block">
                <h3 class="subsection-title">4. Ablation Study</h3>
                <p>Validating the contribution of each component (Diffusion FDM, Feature Differencing, Inverse Consistency, VICReg).</p>
                <img src="images/table5_ablation.png" alt="Table 5: Ablation Study" class="img-fluid" style="max-width: 50%;">
            </div>
        </section>

        <!-- Citation -->
        <section id="citation">
            <div class="text-center"><h2 class="section-title">Citation</h2></div>
            <div class="bibtex-container">
<code>@article{liang2025afro,
  title={Bootstrap Dynamic-Aware 3D Visual Representation for Scalable Robot Learning},
  author={Liang, Qiwei and Cai, Boyang and Lai, Minghao and Zhuang, Sitong and Lin, Tao and Qin, Yan and Liang, Jiaming and Xu, Renjing and Ye, Yixuan},
  journal={arXiv preprint},
  year={2025}
}</code>
            </div>
        </section>

    </div>

    <footer class="text-center">
        <div class="container">
            <p>Copyright © 2025 AFRO Project Team.</p>
        </div>
    </footer>

</body>
</html>